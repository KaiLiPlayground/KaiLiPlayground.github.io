<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AWS Lambda ETL</title>
    <url>/2023/12/19/AWSLambdaETL/</url>
    <content><![CDATA[<h1 id="Triggering-an-AWS-Lambda-function-when-a-new-file-arrives-in-an-S3-bucket"><a href="#Triggering-an-AWS-Lambda-function-when-a-new-file-arrives-in-an-S3-bucket" class="headerlink" title="Triggering an AWS Lambda function when a new file arrives in an S3 bucket"></a>Triggering an AWS Lambda function when a new file arrives in an S3 bucket</h1><h3 id="Creating-a-Lambda-layer-containeing-the-AWS-Data-Wrangler-library"><a href="#Creating-a-Lambda-layer-containeing-the-AWS-Data-Wrangler-library" class="headerlink" title="Creating a Lambda layer containeing the AWS Data Wrangler library"></a>Creating a Lambda layer containeing the AWS Data Wrangler library</h3><h4 id="Layer-Creation"><a href="#Layer-Creation" class="headerlink" title="Layer Creation"></a>Layer Creation</h4><p><img src="/2023/12/19/AWSLambdaETL/layerCreation.png"></p>
<ul>
<li>upload <a class="link"   href="https://github.com/aws/aws-sdk-pandas/releases" >awsdatawrangler<i class="fas fa-external-link-alt"></i></a> to AWS layer and name it as <code>awsDataWrangler342_py38</code></li>
</ul>
<h4 id="Create-two-new-Amazon-S3-buckets"><a href="#Create-two-new-Amazon-S3-buckets" class="headerlink" title="Create two new Amazon S3 buckets"></a>Create two new Amazon S3 buckets</h4><ul>
<li><p>landing zone S3 bucket: <code>dataeng-landing-zone-etl</code></p>
</li>
<li><p>clean zone S3 bucket: <code>dataeng-clean-zone-etl</code></p>
</li>
</ul>
<h4 id="Create-an-IAM-policy-and-role-for-your-Lambda-function"><a href="#Create-an-IAM-policy-and-role-for-your-Lambda-function" class="headerlink" title="Create an IAM policy and role for your Lambda function"></a>Create an IAM policy and role for your Lambda function</h4><ul>
<li><p>name the policy as <code>DataEngLambdaS3CWGluePolicy</code> </p>
</li>
<li><p>Note: remember to replace the two storage names to your own ones</p>
</li>
</ul>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;Version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2012-10-17&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;Statement&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;logs:PutLogEvents&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;logs:CreateLogGroup&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;logs:CreateLogStream&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;arn:aws:logs:*:*:*&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;s3:*&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;arn:aws:s3:::dataeng-landing-zone-etl/*&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;arn:aws:s3:::dataeng-landing-zone-etl&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;arn:aws:s3:::dataeng-clean-zone-etl/*&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;arn:aws:s3:::dataeng-clean-zone-etl&quot;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;glue:*&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Create a custom role and name it as <code>DataEngLambdaS3CWGluePolicy</code> using <code>DataEngLambdaS3CWGluePolicy</code></li>
</ul>
<h4 id="Create-a-Lambda-function"><a href="#Create-a-Lambda-function" class="headerlink" title="Create a Lambda function"></a>Create a Lambda function</h4><ul>
<li>specify the Python version and your custom role in this function, see the below screenshot for reference</li>
<li>name it as <code>CSVtoParquetLambda</code></li>
</ul>
<p><img src="/2023/12/19/AWSLambdaETL/csvtoparquetlambda.png"></p>
<ul>
<li><p>add the layer (<code>awsDataWrangler342_py38</code>) created into the lambda function</p>
</li>
<li><p>add the code source which is given as below</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> boto3</span><br><span class="line"><span class="keyword">import</span> awswrangler <span class="keyword">as</span> wr</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> unquote_plus</span><br><span class="line"></span><br><span class="line"><span class="comment"># this &#x27;debug&#x27; logging part can be removed as it is </span></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.getLogger(<span class="string">&#x27;awswrangler&#x27;</span>).setLevel(logging.DEBUG)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lambda_handler</span>(<span class="params">event, context</span>):</span><br><span class="line">    <span class="comment"># Get the source bucket and object name as passed to the Lambda function</span></span><br><span class="line">    <span class="keyword">for</span> record <span class="keyword">in</span> event[<span class="string">&#x27;Records&#x27;</span>]:</span><br><span class="line">        bucket = record[<span class="string">&#x27;s3&#x27;</span>][<span class="string">&#x27;bucket&#x27;</span>][<span class="string">&#x27;name&#x27;</span>]</span><br><span class="line">        key = unquote_plus(record[<span class="string">&#x27;s3&#x27;</span>][<span class="string">&#x27;object&#x27;</span>][<span class="string">&#x27;key&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We will set the DB and table name based on the last two elements of </span></span><br><span class="line">    <span class="comment"># the path prior to the file name. If key = &#x27;dms/sakila/film/LOAD01.csv&#x27;,</span></span><br><span class="line">    <span class="comment"># then the following lines will set db to sakila and table_name to &#x27;film&#x27;</span></span><br><span class="line">    key_list = key.split(<span class="string">&quot;/&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;key_list: <span class="subst">&#123;key_list&#125;</span>&#x27;</span>)</span><br><span class="line">    db_name = key_list[<span class="built_in">len</span>(key_list)-<span class="number">3</span>]</span><br><span class="line">    table_name = key_list[<span class="built_in">len</span>(key_list)-<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Bucket: <span class="subst">&#123;bucket&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Key: <span class="subst">&#123;key&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;DB Name: <span class="subst">&#123;db_name&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Table Name: <span class="subst">&#123;table_name&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    input_path = <span class="string">f&quot;s3://<span class="subst">&#123;bucket&#125;</span>/<span class="subst">&#123;key&#125;</span>&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Input_Path: <span class="subst">&#123;input_path&#125;</span>&#x27;</span>)</span><br><span class="line">    output_path = <span class="string">f&quot;s3://dataeng-clean-zone-etl/<span class="subst">&#123;db_name&#125;</span>/<span class="subst">&#123;table_name&#125;</span>&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Output_Path: <span class="subst">&#123;output_path&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    input_df = wr.s3.read_csv([input_path])</span><br><span class="line"></span><br><span class="line">    current_databases = wr.catalog.databases()</span><br><span class="line">    wr.catalog.databases()</span><br><span class="line">    <span class="keyword">if</span> db_name <span class="keyword">not</span> <span class="keyword">in</span> current_databases.values:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;- Database <span class="subst">&#123;db_name&#125;</span> does not exist ... creating&#x27;</span>)</span><br><span class="line">        wr.catalog.create_database(db_name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;- Database <span class="subst">&#123;db_name&#125;</span> already exists&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    result = wr.s3.to_parquet(</span><br><span class="line">        df=input_df, </span><br><span class="line">        path=output_path, </span><br><span class="line">        dataset=<span class="literal">True</span>,</span><br><span class="line">        database=db_name,</span><br><span class="line">        table=table_name,</span><br><span class="line">        mode=<span class="string">&quot;append&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;RESULT: &quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;result&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<ul>
<li>Last Step: deploy</li>
</ul>
<blockquote>
<p>Note: </p>
<p><code>logging.getLogger(&#39;awswrangler&#39;).setLevel(logging.DEBUG)</code> is to enable debug log to check if anything can go wrong from the awswrangler library. </p>
</blockquote>
<h4 id="Trigger-configuration"><a href="#Trigger-configuration" class="headerlink" title="Trigger configuration"></a>Trigger configuration</h4><ul>
<li>configurate trigger to listen on landing S3 bucket (<code>dataeng-landing-zone-etl</code>) when a new file (suffix with <code>.csv</code>) arrives</li>
</ul>
<p><img src="/2023/12/19/AWSLambdaETL/triggers.png"></p>
<ul>
<li>create a CSV file in your local file system with the below sample fields and values</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Name,favorite_num</span><br><span class="line">Vrinda,22</span><br><span class="line">Tracy,28</span><br><span class="line">Gareth,23</span><br><span class="line">Chris,16</span><br><span class="line">Emma,14</span><br><span class="line">Carlos,7</span><br><span class="line">Cooper,11</span><br><span class="line">Praful,4</span><br><span class="line">David,33</span><br><span class="line">Shilpa,2</span><br><span class="line">Gary,18</span><br><span class="line">Sean,20</span><br><span class="line">Ha-yoon,9</span><br><span class="line">Elizabeth,8</span><br><span class="line">Mary,1</span><br><span class="line">Chen,15</span><br><span class="line">Janet,22</span><br><span class="line">Mariusz,25</span><br><span class="line">Romain,11</span><br><span class="line">Matt,25</span><br><span class="line">Brendan,19</span><br><span class="line">Roger,2</span><br><span class="line">Jack,7</span><br><span class="line">Sachin,17</span><br><span class="line">Francisco,5</span><br></pre></td></tr></table></figure>

<ul>
<li>upload the CSV file to the remote bucket <code>dataeng-landing-zone-etl</code> using <code>aws</code> CLI</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">aws s3 cp .\test.csv s3://dataeng-landing-zone-etl/testdb/csvparquet/test.csv</span><br></pre></td></tr></table></figure>

<ul>
<li>list the parquet file in the clean zone bucket <code>dataeng-clean-zone-etl</code> if everything done correctly</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">aws s3 ls s3://dataeng-clean-zone-etl/testdb/csvparquet/</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># expected output</span></span> </span><br><span class="line">2023-12-19 16:03:57       2199 d5d09abfbfda4117b544a0bca1f8a134.snappy.parquet</span><br></pre></td></tr></table></figure>

<h4 id="Possible-Issues"><a href="#Possible-Issues" class="headerlink" title="Possible Issues"></a>Possible Issues</h4><ul>
<li>Lamdba function execution timeout shown in CloudWatch log</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1702972039080,&quot;[DEBUG]    2023-12-19T07:47:19.080Z    Applying default config argument verify with value None.</span><br><span class="line">&quot;</span><br><span class="line">1702972040916,&#x27;&quot;- Database testdb already exists</span><br><span class="line">&quot;</span><br><span class="line">1702972040937,&quot;[DEBUG]    2023-12-19T07:47:20.937Z    Applying default config argument botocore_config with value None.</span><br><span class="line">&quot;</span><br><span class="line">1702972040937,&quot;[DEBUG]    2023-12-19T07:47:20.937Z    Applying default config argument verify with value None.</span><br><span class="line">&quot;</span><br><span class="line">1702972043538,&quot;[DEBUG]    2023-12-19T07:47:23.518Z    Applying default config argument botocore_config with value None.</span><br><span class="line">&quot;</span><br><span class="line">1702972043538,&quot;[DEBUG]    2023-12-19T07:47:23.538Z    Applying default config argument verify with value None.</span><br><span class="line">&quot;</span><br><span class="line">1702972214725,&quot;2023-12-19T07:50:14.725Z Task timed out after 182.10 seconds</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Based on the code in the Lambda function, we know Database <code>testdb</code> exists is harmless and expected. But the execution got timed out afterwards. Initially, the function was set at 3 minutes, although the CSV file was only 128 bytes in my case, it still got timed out. </p>
</blockquote>
<ul>
<li><p>Solution</p>
<ul>
<li>extend the execution timeout in the Lambda function in <strong>Configuration</strong> to 10 minutes</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2023-12-19T16:03:53.249+08:00    START RequestId: 67114d4d-25cf-42f2-xxx6e6 Version: $LATEST </span><br><span class="line"></span><br><span class="line"># omitted the intermediate lines </span><br><span class="line"></span><br><span class="line">2023-12-19T16:03:56.713+08:00    &#123;&#x27;paths&#x27;: [&#x27;s3://dataeng-clean-zone-etl/testdb/csvparquet/d5d09abfbfda4117b544a0bca1f8a134.snappy.parquet&#x27;], &#x27;partitions_values&#x27;: &#123;&#125;&#125;</span><br><span class="line">2023-12-19T16:03:56.715+08:00    END RequestId: 67114d4d-25cf-42f2-be60-xxx6e6 </span><br></pre></td></tr></table></figure>

<blockquote>
<p>After examing the time duration for the new Lambda run based on the 10-minutes timeout Lambda definition, our Lambda run completed without error. </p>
<p>And the time duration for the run went for over 3 minutes. That is why our earlier run duration indeed went over the duration of 3 minutes timeout limit. </p>
<p>One possible reason for this is the latency for establishing IO across buckets in AWS. </p>
<p>Although the debug logging level set did not help in this case for troubleshooting, but it could be a great use for future debugging scenarios if something goes wrong with the custom Python script. </p>
</blockquote>
]]></content>
      <categories>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>AWS Lambda</tag>
        <tag>ETL</tag>
      </tags>
  </entry>
  <entry>
    <title>Databricks Certification</title>
    <url>/2023/12/28/DatabricksCertification/</url>
    <content><![CDATA[<h1 id="Introduction-to-Databricks-Certified-Associate-Developer-for-Apache-Spark-3-0"><a href="#Introduction-to-Databricks-Certified-Associate-Developer-for-Apache-Spark-3-0" class="headerlink" title="Introduction to Databricks Certified Associate Developer for Apache Spark 3.0"></a>Introduction to Databricks Certified Associate Developer for Apache Spark 3.0</h1><p>The <strong>Databricks Certified Associate Developer for Apache Spark 3.0</strong> is a prestigious certification designed for professionals seeking to demonstrate their expertise in Apache Spark, particularly within the Databricks platform. This certification is a benchmark for developers who specialize in Spark, emphasizing their ability to efficiently process and analyze large-scale data. It not only validates an individual’s technical skills in Spark application development, Spark SQL, data frames, and performance optimization but also highlights their proficiency in leveraging Databricks for enhanced data processing. As Spark continues to be a leading platform in big data analytics, this certification offers Spark engineers a competitive edge in the job market, ensuring their skills are recognized and valued in this rapidly evolving field.</p>
<h1 id="Proof-of-work"><a href="#Proof-of-work" class="headerlink" title="Proof of work"></a>Proof of work</h1><p>Here is my certificate in screenshot: </p>
<p><img src="/2023/12/28/DatabricksCertification/databricksSparkCertification.png"></p>
]]></content>
      <categories>
        <category>Databricks Spark Certification</category>
      </categories>
      <tags>
        <tag>Databricks Certification</tag>
      </tags>
  </entry>
  <entry>
    <title>Data Migration</title>
    <url>/2023/12/27/DataMigration/</url>
    <content><![CDATA[<h1 id="Ingesting-Data-with-AWS-DMS"><a href="#Ingesting-Data-with-AWS-DMS" class="headerlink" title="Ingesting Data with AWS DMS"></a>Ingesting Data with AWS DMS</h1><p>Following <a class="link"   href="https://subscription.packtpub.com/book/data/9781804614426/8/ch08lvl1sec51/hands-on-ingesting-data-with-aws-dms?_gl=1*1eoz65u*_gcl_au*MTI5MjU4OTg2MC4xNzAzNDg0Mzg5*_ga*MjExODIxOTUyMS4xNzAzNDg0Mzg5*_ga_Q4R8G7SJDK*MTcwMzY1NDkwNC4yLjEuMTcwMzY1NDkyOC4zNi4wLjA." >Ingesting Data with AWS DMS<i class="fas fa-external-link-alt"></i></a>, we learned how to set up the data ingestion with DMS by creating a MySQL SQL instance, EC2 instance for data population, the required access policy and role creation, and Database migration tasks. </p>
<p>Here is a flowchart for the setup:<br><img src="/2023/12/27/DataMigration/AWSServiceSetupFlowchart.png"></p>
<ol>
<li>Create a MySQL instance </li>
<li>Create an EC2 instance </li>
<li>Create an IAM policy and role for DMS</li>
<li>DMS setup for a full load from MySQL to S3</li>
<li>Query data with Athena</li>
</ol>
<p>However, the setup is not a smooth ride as the book I used published in 2021 and it has missed some pivotal steps. Here, I will point out the pivot step and the troubleshooting steps that I used to resolve the issue. </p>
<h2 id="Issue"><a href="#Issue" class="headerlink" title="Issue"></a>Issue</h2><p>The main issue lies in <strong>step 1</strong> and <strong>step 2</strong> that the data population scripts were not taking effect when it tried to load data into the MySQL server. </p>
<h3 id="Issue-Behavior"><a href="#Issue-Behavior" class="headerlink" title="Issue Behavior"></a>Issue Behavior</h3><h4 id="Data-Population-Script"><a href="#Data-Population-Script" class="headerlink" title="Data Population Script"></a>Data Population Script</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">yum install -y mariadb</span><br><span class="line">curl https://downloads.mysql.com/docs/sakila-db.zip -o sakila.zip</span><br><span class="line">unzip sakila.zip</span><br><span class="line">cd sakila-db</span><br><span class="line">mysql --host=HOST --user=admin --password=PASSWORD -f &lt; sakila-schema.sql</span><br><span class="line">mysql --host=HOST --user=admin --password=PASSWORD -f &lt; sakila-data.sql</span><br></pre></td></tr></table></figure>

<p>The script downloads the data population sql script from MySQL site then runs them against the MySQL instance endpoint. </p>
<p>Normally, the next step is to check if MySQL has the Database and tables by connecting to MySQL server. However, I found that the connection was rejected from the EC2 instance. Furthermore, we can check the TCP connection with <code>telnet</code> for which we have: </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[ec2-user@ip-172-31-15-160 ~]$ telnet database-1.cgppbdvzhe7r.ap-southeast-2.rds.amazonaws.com 3306</span><br><span class="line">Trying 172.31.42.75...</span><br><span class="line">telnet: connect to address 172.31.42.75: Connection timed out</span><br></pre></td></tr></table></figure>

<p>As per the steps, we can also verify from the Athena side to check which tables are finally getting catalogged (remember: we have a lambda function set up in <a href="/2023/12/19/AWSLambdaETL/" title="AWS Lambda ETL">AWS Lambda ETL</a> which capture newly loaded CSV in landing zone S3 bucket and convert it to parquet and loads it in clean zone S3 bucket by which new files will get catalogged). </p>
<p>By checking Athena query editor, we can see that data was not found, see the below image:<br><img src="/2023/12/27/DataMigration/preNSGwhitelist.png"></p>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>According the behavior discovered, we know the steps of the book section missed an important validation. That is, the network connection has yet confirmed between EC2 and MySQL server instance. </p>
<p>Since both MySQL instance and EC2 intance are using the same AWS managed VPC, we can set up two security groups to connect the MySQL database and EC2 instance.<br><img src="/2023/12/27/DataMigration/ec2_connection.png"> </p>
<p>After setting up the connection in VPC for the two services by adding security group rules, we need to run the data population script to load the data to MySQL instance in EC2. </p>
<h3 id="Validation"><a href="#Validation" class="headerlink" title="Validation"></a>Validation</h3><ul>
<li><p>Check MySQL server databases</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">  MySQL [(none)]&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| mysql              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| sakila             | # sakila is loaded as intended</span><br><span class="line">| sys                |</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure>
</li>
<li><p>check S3 landing and clean buckets</p>
<ul>
<li><p>landing zone (CSV files)<br><img src="/2023/12/27/DataMigration/s3_landing_zone.png"></p>
</li>
<li><p>landing zone <code>actor</code> folder (pick one for reference)<br><img src="/2023/12/27/DataMigration/landing_actor_src_csv.png"></p>
</li>
<li><p>clean zone (parquet files)<br><img src="/2023/12/27/DataMigration/clean_zone.png"> </p>
</li>
<li><p>clean zone <code>actor</code> folder (pick one for reference)</p>
</li>
</ul>
<ul>
<li>check Athena query editor to valid if all files are catalogged<br><img src="/2023/12/27/DataMigration/data_populated.png"></li>
</ul>
</li>
</ul>
<p>The file changes capturing in landing zone and conversion in clean zone along with catalogging is set up in the Lambda function in <a href="/2023/12/19/AWSLambdaETL/" title="AWS Lambda ETL">AWS Lambda ETL</a>. </p>
]]></content>
      <categories>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>AWS Ingestion</tag>
        <tag>AWS DMS Migration</tag>
      </tags>
  </entry>
  <entry>
    <title>I94 Transform Analytics ETL - Proj. 2</title>
    <url>/2024/02/01/I94TransformAnalyticsETL/</url>
    <content><![CDATA[<h1 id="Automating-Data-Transformation-in-Apache-Airflow-The-i94-Data-Transformation-Case"><a href="#Automating-Data-Transformation-in-Apache-Airflow-The-i94-Data-Transformation-Case" class="headerlink" title="Automating Data Transformation in Apache Airflow: The i94 Data Transformation Case"></a>Automating Data Transformation in Apache Airflow: The i94 Data Transformation Case</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In this post, we’ll dive into a practical example: the <code>i94_data_transformation</code> DAG. This Airflow DAG automates the transformation of i94 immigration data, leveraging Python scripts for each task and integrating them into an existing DAG structure.</p>
<h2 id="Constructing-the-DAG"><a href="#Constructing-the-DAG" class="headerlink" title="Constructing the DAG"></a>Constructing the DAG</h2><p>We began by creating specific Python scripts for each data transformation task. These scripts are designed to be executed within an EMR cluster environment, using Apache Spark through Livy, a service that enables remote job submission.</p>
<p>This project is built upon the environment basis of <a href="/2024/01/31/MovieLensProj/" title="MovieLens-Project 2">MovieLens-Project 2</a></p>
<p>Each script is placed in the <code>/root/airflow/dags/I94analytics/</code> directory, following a structured naming convention (e.g., <code>normalize_dag_check.py</code>, <code>transform_immigration.py</code>). This organization makes it easy to manage and update the scripts independently.</p>
<p>Here’s a representative portion of the script setup:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># i94_data_transformation task scripts</span></span><br><span class="line"><span class="comment"># /root/airflow/dags/I94analytics/normalize_dag_check.py</span></span><br><span class="line"><span class="comment"># /root/airflow/dags/I94analytics/transform_immigration.py</span></span><br><span class="line"><span class="comment"># /root/airflow/dags/I94analytics/transform_immigration_demographics.py</span></span><br><span class="line"><span class="comment"># /root/airflow/dags/I94analytics/quality_check.py</span></span><br><span class="line"><span class="comment"># ... additional task scripts ...</span></span><br></pre></td></tr></table></figure>
<p>All transformation tasks scripts can be found at <a class="link"   href="https://github.com/KaiLiPlayground/i94_data_transformation_proj/tree/main/transform" >normalized scripts<i class="fas fa-external-link-alt"></i></a> </p>
<h2 id="Integrating-Scripts-into-the-Airflow-DAG"><a href="#Integrating-Scripts-into-the-Airflow-DAG" class="headerlink" title="Integrating Scripts into the Airflow DAG"></a>Integrating Scripts into the Airflow DAG</h2><p>The next step was to integrate these scripts into an existing Airflow DAG. We used the <code>PythonOperator</code> to define each task, dynamically creating them based on the script names. This approach not only streamlined the DAG construction but also offered flexibility and scalability.</p>
<p>Here’s a snippet of the DAG definition:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Import necessary modules</span></span><br><span class="line"><span class="keyword">import</span> airflowlib.emr_lib <span class="keyword">as</span> emr</span><br><span class="line"><span class="keyword">from</span> airflow <span class="keyword">import</span> DAG</span><br><span class="line"><span class="keyword">from</span> airflow.operators.python_operator <span class="keyword">import</span> PythonOperator</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime, timedelta</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define default args</span></span><br><span class="line">default_args = &#123;</span><br><span class="line">    <span class="string">&#x27;owner&#x27;</span>: <span class="string">&#x27;airflow&#x27;</span>,</span><br><span class="line">    <span class="comment"># ... additional default args ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the DAG</span></span><br><span class="line">dag = DAG(<span class="string">&#x27;i94_data_transformation&#x27;</span>, default_args=default_args, schedule_interval=<span class="literal">None</span>, concurrency=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dynamically create tasks</span></span><br><span class="line">task_names = [<span class="string">&quot;normalize_dag_check&quot;</span>, <span class="string">&quot;transform_immigration&quot;</span>, ...]</span><br><span class="line"><span class="keyword">for</span> task_name <span class="keyword">in</span> task_names:</span><br><span class="line">    <span class="built_in">globals</span>()[task_name] = PythonOperator(</span><br><span class="line">        task_id=task_name,</span><br><span class="line">        python_callable=submit_spark_job,</span><br><span class="line">        op_kwargs=&#123;<span class="string">&#x27;task_script&#x27;</span>: <span class="string">f&#x27;<span class="subst">&#123;task_name&#125;</span>.py&#x27;</span>&#125;,</span><br><span class="line">        dag=dag</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define task dependencies</span></span><br><span class="line">create_cluster &gt;&gt; wait_for_cluster_completion</span><br><span class="line">wait_for_cluster_completion &gt;&gt; normalize_dag_check</span><br><span class="line"><span class="comment"># ... additional dependencies ...</span></span><br></pre></td></tr></table></figure>

<p>The full I94TransformationAnalyticsETL dag can be found at <a class="link"   href="https://github.com/KaiLiPlayground/i94_data_transformation_proj/blob/main/dag/i94_data_transformation_dag.py" >i94_data_transformation_dag.py<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>The <code>i94_data_transformation</code> DAG represents a robust solution for automating the data transformation process in Apache Airflow. By separating the task logic into individual Python scripts and dynamically creating tasks in the DAG, we achieved a modular and scalable workflow. This approach simplifies maintenance and updates, allowing data engineers to focus on optimizing individual parts of the process without altering the overall DAG structure.</p>
<p>Note: the project is inspired by <a class="link"   href="https://danieldiamond.github.io/deng-capstone/" >Udacity Capstone Project - Daniel’s post<i class="fas fa-external-link-alt"></i></a> </p>
]]></content>
      <categories>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>AWS Lambda</tag>
        <tag>AWS EMR</tag>
        <tag>AWS Airflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Managing Fine-grained Permission with AWS Lake Formation</title>
    <url>/2023/12/25/LakeFormationNew/</url>
    <content><![CDATA[<h1 id="Granting-Lake-Formation-Permissions"><a href="#Granting-Lake-Formation-Permissions" class="headerlink" title="Granting Lake Formation Permissions"></a>Granting Lake Formation Permissions</h1><h3 id="Prerequite"><a href="#Prerequite" class="headerlink" title="Prerequite"></a>Prerequite</h3><p>Following the instruction of <a class="link"   href="https://subscription.packtpub.com/book/data/9781804614426/5/ch05lvl1sec33/hands-on-configuring-lake-formation-permissions" >LakeFormation permissions<i class="fas fa-external-link-alt"></i></a>, the intended IAM policies to manage fine-grained access control for data lake has been configured. We now use transition to using Lake Formation to manage fine-grained permission on data lake object. </p>
<h2 id="Motivation-and-Issue"><a href="#Motivation-and-Issue" class="headerlink" title="Motivation and Issue"></a>Motivation and Issue</h2><p>Our goal is to experiment with the granular permission data acess on the column level access of a table in Lake Formation. </p>
<p>At the time of writing this post, the data filter row access UI button is not working, see the below image for reference: </p>
<p><img src="/2023/12/25/LakeFormationNew/dataFilter.png" alt="datafilter"></p>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>We can use AWS CLI as a workaround to UI button issue to allow principal user accessing one specifc column only while blocking the others and allow access to all rows. Here the sample AWS CLI command: </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">aws lakeformation grant-permissions \</span><br><span class="line">--principal DataLakePrincipalIdentifier=arn:aws:iam::&lt;account-id&gt;:user/database-user&quot; \</span><br><span class="line">--resource &#x27;&#123;&quot;TableWithColumns&quot;:&#123;&quot;DatabaseName&quot;:&quot;testdb&quot;,&quot;Name&quot;:&quot;csvparquet&quot;,&quot;ColumnNames&quot;:[&quot;name&quot;]&#125;&#125;&#x27; \</span><br><span class="line">--permissions &quot;SELECT&quot; \</span><br><span class="line">--permissions-with-grant-option &quot;SELECT&quot; \</span><br><span class="line">--profile &quot;&lt;Profile Name&gt;&quot;</span><br></pre></td></tr></table></figure>

<p>In this command, we are allowing the specified principal user to have access to column <code>name</code> but restricting others. The <code>csvparquet</code> table has columns, <code>name</code> and <code>favorite_num</code>. The expected output for querying against the table using the specified principal user is that only column <code>name</code> is showing, see the below image for reference: </p>
<p><img src="/2023/12/25/LakeFormationNew/athenaquery.png"></p>
<hr>
]]></content>
      <categories>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>AWS LakeFormation</tag>
        <tag>Fine-grained Permission</tag>
      </tags>
  </entry>
  <entry>
    <title>MovieLens-Project 2</title>
    <url>/2024/01/31/MovieLensProj/</url>
    <content><![CDATA[<h1 id="Building-an-Automated-Data-Pipeline-with-AWS-Airflow-and-Redshift-—-Proj-2"><a href="#Building-an-Automated-Data-Pipeline-with-AWS-Airflow-and-Redshift-—-Proj-2" class="headerlink" title="Building an Automated Data Pipeline with AWS, Airflow, and Redshift — Proj. 2"></a>Building an Automated Data Pipeline with AWS, Airflow, and Redshift — Proj. 2</h1><p>In this post, I’ll walk you through setting up an automated data pipeline using AWS CloudFormation, Apache Airflow, Amazon EMR, and Amazon Redshift. This pipeline converts CSV files to Parquet format and then loads them into a Redshift data warehouse for further data modeling and scalability. </p>
<p>We went above and beyond the original <a class="link"   href="https://aws.amazon.com/blogs/big-data/build-a-concurrent-data-orchestration-pipeline-using-amazon-emr-and-apache-livy/" >blog post<i class="fas fa-external-link-alt"></i></a> where the author has outdated scripts and configuration errors and plust the services in cloudformation setup are not up to date. In addition, we have also added data catalog to allow Athena query along with the idea of sending the parquet data to redshift. </p>
<h2 id="1-AWS-CloudFormation-Template-for-Infrastructure-Setup"><a href="#1-AWS-CloudFormation-Template-for-Infrastructure-Setup" class="headerlink" title="1. AWS CloudFormation Template for Infrastructure Setup"></a>1. AWS CloudFormation Template for Infrastructure Setup</h2><p>We start by defining the necessary infrastructure using an AWS CloudFormation template. This includes setting up an EC2 instance, Postgres RDS for the Airflow metastore, S3 bucket, and the necessary roles.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">AWSTemplateFormatVersion:</span> <span class="string">&#x27;2010-09-09&#x27;</span></span><br><span class="line"><span class="attr">Description:</span> <span class="string">Airflow</span> <span class="string">server</span> <span class="string">backed</span> <span class="string">by</span> <span class="string">Postgres</span> <span class="string">RDS</span> <span class="string">with</span> <span class="string">VPC</span> <span class="string">and</span> <span class="string">subnets</span></span><br><span class="line"></span><br><span class="line"><span class="attr">Parameters:</span></span><br><span class="line">  <span class="string">...</span> [<span class="string">parameters</span> <span class="string">here</span>] <span class="string">...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">Mappings:</span></span><br><span class="line">  <span class="string">...</span> [<span class="string">AMI</span> <span class="string">mappings</span> <span class="string">here</span>] <span class="string">...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">Resources:</span></span><br><span class="line">  <span class="string">...</span> [<span class="string">resource</span> <span class="string">definitions</span> <span class="string">here</span>] <span class="string">...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">Outputs:</span></span><br><span class="line">  <span class="string">...</span> [<span class="string">outputs</span> <span class="string">here</span>] <span class="string">...</span></span><br></pre></td></tr></table></figure>

<p>The full cloudformation template can be accessed at <a class="link"   href="https://github.com/KaiLiPlayground/MovieLens_ETL_Proj/blob/main/movie_etl_proj/airflow/cloudformation/airflow.yaml" >airflow3.yml<i class="fas fa-external-link-alt"></i></a>.</p>
<p>Here is how it looks like when all resources are deployed successfully</p>
<p><img src="/2024/01/31/MovieLensProj/airflow3_c_stack.png"></p>
<h2 id="Airflow-service-setup"><a href="#Airflow-service-setup" class="headerlink" title="Airflow service setup"></a>Airflow service setup</h2><p>The following commands included for service installation, Airflow scheduler start up and Airflow webserver start up, Airflow admin user creation, and Airflow services shut down for services restart. </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo su</span><br><span class="line"></span><br><span class="line">mkdir -p ~/p2/airflow</span><br><span class="line"></span><br><span class="line">python3 -m venv ~/p2/airflow/venv</span><br><span class="line">source ~/p2/airflow/venv/bin/activate</span><br><span class="line"></span><br><span class="line">AIRFLOW_VERSION=2.8.1</span><br><span class="line">PYTHON_VERSION=&quot;$(python3 --version | cut -d &quot; &quot; -f 2 | cut -d &quot;.&quot; -f 1-2)&quot;</span><br><span class="line">CONSTRAINT_URL=&quot;https://raw.githubusercontent.com/apache/airflow/constraints-$&#123;AIRFLOW_VERSION&#125;/constraints-$&#123;PYTHON_VERSION&#125;.txt&quot;</span><br><span class="line">pip install &quot;apache-airflow[crypto,postgres]==$&#123;AIRFLOW_VERSION&#125;&quot; --constraint &quot;$&#123;CONSTRAINT_URL&#125;&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Additional dependencies <span class="keyword">if</span> needed</span></span><br><span class="line">sudo pip install six --upgrade</span><br><span class="line">sudo pip install markupsafe --upgrade</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Set up environment variables</span></span><br><span class="line">echo &#x27;export PATH=/usr/local/bin:$PATH&#x27; &gt;&gt; /root/.bash_profile</span><br><span class="line">source /root/.bash_profile</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Initialize Airflow</span></span><br><span class="line">airflow initdb</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Configure Airflow settings</span></span><br><span class="line">sed -i &#x27;/sql_alchemy_conn/s/^/#/g&#x27; ~/airflow/airflow.cfg</span><br><span class="line">sed -i &#x27;/sql_alchemy_conn/c\sql_alchemy_conn = postgresql://airflow:airflowpassword@airflowstack2024-dbinstance-vc9uopllco1y.cgppbdvzhe7r.ap-southeast-2.rds.amazonaws.com:5432/airflowdb</span><br><span class="line">sed -i &#x27;/executor = SequentialExecutor/s/^/#/g&#x27; ~/airflow/airflow.cfg</span><br><span class="line">sed -i &#x27;/executor = SequentialExecutor/ a executor = LocalExecutor&#x27; ~/airflow/airflow.cfg</span><br><span class="line">airflow initdb</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">install awscli</span> </span><br><span class="line">pip install --upgrade --user awscli</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Download and unzip the Movielens dataset</span></span><br><span class="line">wget http://files.grouplens.org/datasets/movielens/ml-latest.zip &amp;&amp; unzip ml-latest.zip</span><br><span class="line"></span><br><span class="line">aws s3 cp ml-latest s3://kai-airflow-storage --recursive</span><br><span class="line"></span><br><span class="line">sudo pip install boto3</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="keyword">for</span> all installation of py libraries, make sure to clear the conflicts by checking <span class="string">&quot;pip check&quot;</span></span></span><br><span class="line"></span><br><span class="line">apt install -y git</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Clone the git repository</span></span><br><span class="line">git clone https://github.com/aws-samples/aws-concurrent-data-orchestration-pipeline-emr-livy.git</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash">Move all the files to the ~/airflow directory. The Airflow config file is setup to hold all the DAG related files <span class="keyword">in</span> the ~/airflow/ folder.</span></span><br><span class="line">mv aws-concurrent-data-orchestration-pipeline-emr-livy/* ~/airflow/</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Delete the higher-level git repository directory</span></span><br><span class="line">rm -rf aws-concurrent-data-orchestration-pipeline-emr-livy</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Replace the name of the S3 bucket <span class="keyword">in</span> each of the .scala files. CHANGE THE HIGHLIGHTED PORTION BELOW TO THE NAME OF THE S3 BUCKET YOU CREATED IN STEP 1. The below <span class="built_in">command</span> replaces the instance of the string ‘&lt;s3-bucket&gt;’ <span class="keyword">in</span> each of the scripts to the name of the actual bucket.</span></span><br><span class="line">sed -i &#x27;s/&lt;s3-bucket&gt;/kai-airflow-storage/g&#x27; /root/airflow/dags/transform/*</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">run airflow scheduler to start the DAGs</span></span><br><span class="line">airflow scheduler</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Run Airflow webserver</span></span><br><span class="line">airflow webserver</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">set</span> user and password once airflow is up and running</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">airflow <span class="built_in">users</span> create \</span></span><br><span class="line"><span class="language-bash"><span class="comment">#    --username admin \</span></span></span><br><span class="line"><span class="language-bash"><span class="comment">#    --firstname FIRST_NAME \</span></span></span><br><span class="line"><span class="language-bash"><span class="comment">#    --lastname LAST_NAME \</span></span></span><br><span class="line"><span class="language-bash"><span class="comment">#    --role Admin \</span></span></span><br><span class="line"><span class="language-bash"><span class="comment">#    --email admin@example.com</span></span></span><br><span class="line">airflow users create \</span><br><span class="line">    --username kaiadmin \</span><br><span class="line">    --firstname Kai \</span><br><span class="line">    --lastname Li \</span><br><span class="line">    --role Admin \</span><br><span class="line">    --email your_email@example.com</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">restart airflow services</span> </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">kill</span> all airflow processes</span> </span><br><span class="line">ps aux | grep &#x27;scheduler&#x27; | awk &#x27;&#123;print $2&#125;&#x27; | xargs -r sudo kill -9</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">kill</span> all worker</span> </span><br><span class="line">lsof -i :8793 | awk &#x27;NR&gt;1 &#123;print $2&#125;&#x27; | xargs sudo kill -9</span><br></pre></td></tr></table></figure>

<p>Note that the above setup is for testing purpose only. In production, you would want to consider deploying the worker services in other EC2 instances for setting a distributive Airflow service. Also, you would want to experiment containerized Airflow services management to avoid the hassle of Python library installation conflicts. </p>
<h2 id="2-Airflow-DAG-for-Lambda-Function-Creation"><a href="#2-Airflow-DAG-for-Lambda-Function-Creation" class="headerlink" title="2. Airflow DAG for Lambda Function Creation"></a>2. Airflow DAG for Lambda Function Creation</h2><p>Next, we define an Airflow DAG (<code>transform_movielens_v2</code>) to set up a Python script for cataloging Parquet files and creating a Lambda function.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> airflowlib.emr_lib <span class="keyword">as</span> emr</span><br><span class="line"><span class="keyword">import</span> os, boto3, logging, zipfile, textwrap</span><br><span class="line"><span class="keyword">from</span> airflow.models <span class="keyword">import</span> Variable</span><br><span class="line"><span class="meta">... </span>[rest of the imports] ...</span><br><span class="line"></span><br><span class="line">default_args = &#123;</span><br><span class="line">    ... [default args here] ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dag = DAG(<span class="string">&#x27;transform_movielens_v2&#x27;</span>, concurrency=<span class="number">3</span>, schedule_interval=<span class="literal">None</span>, default_args=default_args)</span><br><span class="line"></span><br><span class="line"><span class="meta">... </span>[rest of the DAG definition] ...</span><br><span class="line"></span><br><span class="line">create_zip_operator &gt;&gt; create_lambda_function_operator</span><br></pre></td></tr></table></figure>

<p>The full <code>movielens_dag_v2.py</code> script can be found at <a class="link"   href="https://github.com/KaiLiPlayground/MovieLens_ETL_Proj/blob/main/movie_etl_proj/airflow/dags/movielens_dag_v2.py" >movielens_dag_v2.py<i class="fas fa-external-link-alt"></i></a></p>
<p>When both tasks ran in success, you should see the following</p>
<p><img src="/2024/01/31/MovieLensProj/lambda_function_creation_dag_tasks.png"></p>
<h2 id="3-Airflow-DAG-for-CSV-to-Parquet-Conversion"><a href="#3-Airflow-DAG-for-CSV-to-Parquet-Conversion" class="headerlink" title="3. Airflow DAG for CSV to Parquet Conversion"></a>3. Airflow DAG for CSV to Parquet Conversion</h2><p>We then define the main pipeline (<code>transform_movielens_v1</code>) for the CSV to Parquet conversion using EMR.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> airflowlib.emr_lib <span class="keyword">as</span> emr</span><br><span class="line"><span class="meta">... </span>[rest of the imports] ...</span><br><span class="line"></span><br><span class="line">default_args = &#123;</span><br><span class="line">    ... [default args here] ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dag = DAG(<span class="string">&#x27;transform_movielens_v1&#x27;</span>, concurrency=<span class="number">3</span>, schedule_interval=<span class="literal">None</span>, default_args=default_args)</span><br><span class="line"></span><br><span class="line"><span class="meta">... </span>[rest of the DAG definition] ...</span><br><span class="line"></span><br><span class="line">create_cluster &gt;&gt; wait_for_cluster_completion</span><br><span class="line"><span class="meta">... </span>[rest of the DAG dependencies] ...</span><br></pre></td></tr></table></figure>

<p>The full <code>transform_movielens_v1</code> dag can be found at <a class="link"   href="https://github.com/KaiLiPlayground/MovieLens_ETL_Proj/blob/main/movie_etl_proj/airflow/dags/movielens_dag.py" >movielens_dag.py<i class="fas fa-external-link-alt"></i></a></p>
<p>When all tasks ran in success, we should see something as such: </p>
<p><img src="/2024/01/31/MovieLensProj/movie_dag_tasks.png"></p>
<h2 id="4-Extending-the-Pipeline-to-Load-Data-into-Redshift"><a href="#4-Extending-the-Pipeline-to-Load-Data-into-Redshift" class="headerlink" title="4. Extending the Pipeline to Load Data into Redshift"></a>4. Extending the Pipeline to Load Data into Redshift</h2><p>For further data modeling operations, we extend the pipeline to load the converted Parquet files into Redshift.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">create_lambda_zip &gt;&gt; create_lambda_function &gt;&gt; create_cluster &gt;&gt; wait_for_cluster_completion</span><br><span class="line">wait_for_cluster_completion &gt;&gt; transform_movies &gt;&gt; load_to_redshift_movies &gt;&gt; terminate_cluster</span><br><span class="line"><span class="meta">... </span>[rest of the workflow] ...</span><br></pre></td></tr></table></figure>

<p>The Scala code for loading data into Redshift looks like this:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataframe = spark.read.parquet(<span class="string">&quot;s3://kai-airflow-storage/movielens-parquet/&lt;item&gt;&quot;</span>)</span><br><span class="line">dataframe.write</span><br><span class="line">  .format(<span class="string">&quot;io.github.spark_redshift_community.spark.redshift&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:redshift://&lt;redshift-url&gt;:5439/&lt;database&gt;?user=&lt;username&gt;&amp;password=&lt;password&gt;&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;&lt;table-name&gt;&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;tempdir&quot;</span>, <span class="string">&quot;s3://&lt;temp-bucket&gt;/temp&quot;</span>)</span><br><span class="line">  .mode(<span class="string">&quot;overwrite&quot;</span>)</span><br><span class="line">  .save()</span><br></pre></td></tr></table></figure>

<p><em>Note: EMR already contains the Redshift connector.</em></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>This setup automates the process of transforming movie dataset files from CSV to Parquet and then loading them into Redshift for scalable data warehousing and modeling. It demonstrates the power and flexibility of AWS services and Apache Airflow in creating efficient data pipelines.</p>
]]></content>
      <categories>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>AWS Lambda</tag>
        <tag>AWS EMR</tag>
        <tag>AWS Airflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Stream Ingestion</title>
    <url>/2023/12/27/StreamIngestion/</url>
    <content><![CDATA[<h1 id="Streamlining-Data-Processing-with-AWS-Lambda-and-AWS-Data-Wrangler"><a href="#Streamlining-Data-Processing-with-AWS-Lambda-and-AWS-Data-Wrangler" class="headerlink" title="Streamlining Data Processing with AWS Lambda and AWS Data Wrangler"></a>Streamlining Data Processing with AWS Lambda and AWS Data Wrangler</h1><p>When working with <a class="link"   href="https://subscription.packtpub.com/book/data/9781800560413/8/ch08lvl1sec52/hands-on-ingesting-streaming-data?_gl=1*17mtlck*_gcl_au*MTI5MjU4OTg2MC4xNzAzNDg0Mzg5*_ga*MjExODIxOTUyMS4xNzAzNDg0Mzg5*_ga_Q4R8G7SJDK*MTcwMzgyMDk4NC40LjAuMTcwMzgyMDk4NC42MC4wLjA." >Data Engineering with AWS - Ingesting Streaming Data<i class="fas fa-external-link-alt"></i></a>, I encountered a challenge with setting up an AWS Glue Crawler using a specific role. This led me to devise an alternative method for processing and cataloging data in S3, utilizing AWS Lambda and AWS Data Wrangler.</p>
<h2 id="The-Challenge"><a href="#The-Challenge" class="headerlink" title="The Challenge"></a>The Challenge</h2><p>My initial plan was to use an AWS Glue Crawler for schema inference and metadata table creation in the AWS Glue Data Catalog. However, due to issues with the crawler setup and the role configuration, I had to explore other options.</p>
<p>The issue shown in screenshot: </p>
<p><img src="/2023/12/27/StreamIngestion/glue_crawler.png"></p>
<h2 id="Innovative-Solution"><a href="#Innovative-Solution" class="headerlink" title="Innovative Solution"></a>Innovative Solution</h2><p>I shifted my approach to a more dynamic solution involving AWS Lambda and AWS Data Wrangler. Here’s how I did it:</p>
<ol>
<li><p><strong>Lambda Function for Triggering</strong>: I set up an AWS Lambda function to trigger upon file creation in the S3 landing zone bucket.</p>
</li>
<li><p><strong>Data Transformation</strong>: The Lambda function then processes these files, converting them into a more efficient Parquet format.</p>
</li>
<li><p><strong>Storing Processed Data</strong>: Post-conversion, the data is stored in a separate ‘clean zone’ within the S3 bucket.</p>
</li>
<li><p><strong>Cataloging with AWS Data Wrangler</strong>: Finally, I used AWS Data Wrangler to catalog the transformed data, ensuring it was ready for efficient querying and analysis.</p>
</li>
</ol>
<p>The Python script used in the Lambda function</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> boto3</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> awswrangler <span class="keyword">as</span> wr</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd  <span class="comment"># Import pandas</span></span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> unquote_plus</span><br><span class="line"></span><br><span class="line">destination_bucket = <span class="string">&#x27;dataeng-clean-zone-etl&#x27;</span></span><br><span class="line"></span><br><span class="line">s3_client = boto3.client(<span class="string">&#x27;s3&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lambda_handler</span>(<span class="params">event, context</span>):</span><br><span class="line">    <span class="keyword">for</span> record <span class="keyword">in</span> event[<span class="string">&#x27;Records&#x27;</span>]:</span><br><span class="line">        bucket = record[<span class="string">&#x27;s3&#x27;</span>][<span class="string">&#x27;bucket&#x27;</span>][<span class="string">&#x27;name&#x27;</span>]</span><br><span class="line">        key = unquote_plus(record[<span class="string">&#x27;s3&#x27;</span>][<span class="string">&#x27;object&#x27;</span>][<span class="string">&#x27;key&#x27;</span>])</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Bucket: <span class="subst">&#123;bucket&#125;</span>, Key: <span class="subst">&#123;key&#125;</span>&#x27;</span>)</span><br><span class="line">        key_list = key.split(<span class="string">&quot;/&quot;</span>)</span><br><span class="line">        db_name = key_list[<span class="number">0</span>]  <span class="comment"># &#x27;streaming&#x27;</span></span><br><span class="line">        table_name = key_list[<span class="number">3</span>].split(<span class="string">&quot;-&quot;</span>)[<span class="number">0</span>] + <span class="string">&quot;-&quot;</span> + key_list[<span class="number">3</span>].split(<span class="string">&quot;-&quot;</span>)[<span class="number">1</span>] + <span class="string">&quot;-&quot;</span> + key_list[<span class="number">3</span>].split(<span class="string">&quot;-&quot;</span>)[<span class="number">2</span>]  <span class="comment"># &#x27;dataeng-firehose-streaming-s3&#x27;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;DB Name: <span class="subst">&#123;db_name&#125;</span>, Table Name: <span class="subst">&#123;table_name&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># input_path = f&quot;s3://&#123;bucket&#125;/&#123;key&#125;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(f&#x27;input path: &#123;input_path&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">        output_path = <span class="string">f&quot;s3://<span class="subst">&#123;destination_bucket&#125;</span>/<span class="subst">&#123;db_name&#125;</span>/<span class="subst">&#123;table_name&#125;</span>/&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;output path: <span class="subst">&#123;output_path&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Read the file line by line and parse each line as a JSON object </span></span><br><span class="line">        obj = s3_client.get_object(Bucket=bucket, Key=key)</span><br><span class="line">        file_content = obj[<span class="string">&#x27;Body&#x27;</span>].read().decode(<span class="string">&#x27;utf-8&#x27;</span>) </span><br><span class="line">        <span class="comment"># print(f&#x27;file content: &#123;file_content&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        lines = file_content.splitlines()</span><br><span class="line">        records = [json.loads(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">        <span class="comment"># print(f&#x27;records: &#123;records&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert to DataFrame</span></span><br><span class="line">        input_df = pd.DataFrame(records)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> db_name <span class="keyword">not</span> <span class="keyword">in</span> wr.catalog.databases().values:</span><br><span class="line">            wr.catalog.create_database(db_name)</span><br><span class="line"></span><br><span class="line">        wr.s3.to_parquet(</span><br><span class="line">            df=input_df, </span><br><span class="line">            path=output_path, </span><br><span class="line">            dataset=<span class="literal">True</span>,</span><br><span class="line">            database=db_name,</span><br><span class="line">            table=table_name,</span><br><span class="line">            mode=<span class="string">&quot;append&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;records&#x27;</span>: event[<span class="string">&#x27;Records&#x27;</span>]&#125;</span><br></pre></td></tr></table></figure>

<p>The Lambda function definition Yaml: </p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># This AWS SAM template has been generated from your function&#x27;s </span></span><br><span class="line"><span class="comment"># configuration. If your function has one or more triggers, note </span></span><br><span class="line"><span class="comment"># that the AWS resources associated with these triggers aren&#x27;t fully </span></span><br><span class="line"><span class="comment"># specified in this template and include placeholder values.Open this template </span></span><br><span class="line"><span class="comment"># in AWS Application Composer or your favorite IDE and modify </span></span><br><span class="line"><span class="comment"># it to specify a serverless application with other AWS resources. </span></span><br><span class="line"><span class="attr">AWSTemplateFormatVersion:</span> <span class="string">&#x27;2010-09-09&#x27;</span></span><br><span class="line"><span class="attr">Transform:</span> <span class="string">AWS::Serverless-2016-10-31</span></span><br><span class="line"><span class="attr">Description:</span> <span class="string">An</span> <span class="string">AWS</span> <span class="string">Serverless</span> <span class="string">Specification</span> <span class="string">template</span> <span class="string">describing</span> <span class="string">your</span> <span class="string">function.</span></span><br><span class="line"><span class="attr">Resources:</span></span><br><span class="line">  <span class="attr">KinesisFirehoseConsume:</span></span><br><span class="line">    <span class="attr">Type:</span> <span class="string">AWS::Serverless::Function</span></span><br><span class="line">    <span class="attr">Properties:</span></span><br><span class="line">      <span class="attr">CodeUri:</span> <span class="string">.</span></span><br><span class="line">      <span class="attr">Description:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">      <span class="attr">MemorySize:</span> <span class="number">128</span></span><br><span class="line">      <span class="attr">Timeout:</span> <span class="number">600</span></span><br><span class="line">      <span class="attr">Handler:</span> <span class="string">lambda_function.lambda_handler</span></span><br><span class="line">      <span class="attr">Runtime:</span> <span class="string">python3.8</span></span><br><span class="line">      <span class="attr">Architectures:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">x86_64</span></span><br><span class="line">      <span class="attr">EventInvokeConfig:</span></span><br><span class="line">        <span class="attr">MaximumEventAgeInSeconds:</span> <span class="number">21600</span></span><br><span class="line">        <span class="attr">MaximumRetryAttempts:</span> <span class="number">2</span></span><br><span class="line">      <span class="attr">EphemeralStorage:</span></span><br><span class="line">        <span class="attr">Size:</span> <span class="number">512</span></span><br><span class="line">      <span class="attr">Events:</span></span><br><span class="line">        <span class="attr">BucketEvent1:</span></span><br><span class="line">          <span class="attr">Type:</span> <span class="string">S3</span></span><br><span class="line">          <span class="attr">Properties:</span></span><br><span class="line">            <span class="attr">Bucket:</span></span><br><span class="line">              <span class="attr">Ref:</span> <span class="string">Bucket1</span></span><br><span class="line">            <span class="attr">Events:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="string">s3:ObjectCreated:*</span></span><br><span class="line">            <span class="attr">Filter:</span></span><br><span class="line">              <span class="attr">S3Key:</span></span><br><span class="line">                <span class="attr">Rules:</span></span><br><span class="line">                  <span class="bullet">-</span> <span class="attr">Name:</span> <span class="string">prefix</span></span><br><span class="line">                    <span class="attr">Value:</span> <span class="string">streaming/2023/12/</span></span><br><span class="line">      <span class="attr">Layers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&gt;-</span></span><br><span class="line"><span class="string">          arn:aws:lambda:ap-southeast-2:028210085659:layer:awsDataWrangler342_py38:1</span></span><br><span class="line"><span class="string"></span>      <span class="attr">RuntimeManagementConfig:</span></span><br><span class="line">        <span class="attr">UpdateRuntimeOn:</span> <span class="string">Auto</span></span><br><span class="line">      <span class="attr">SnapStart:</span></span><br><span class="line">        <span class="attr">ApplyOn:</span> <span class="string">None</span></span><br><span class="line">      <span class="attr">PackageType:</span> <span class="string">Zip</span></span><br><span class="line">      <span class="attr">Policies:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">Statement:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">Effect:</span> <span class="string">Allow</span></span><br><span class="line">              <span class="attr">Action:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">logs:PutLogEvents</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">logs:CreateLogGroup</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">logs:CreateLogStream</span></span><br><span class="line">              <span class="attr">Resource:</span> <span class="string">arn:aws:logs:*:*:*</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">Effect:</span> <span class="string">Allow</span></span><br><span class="line">              <span class="attr">Action:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">s3:*</span></span><br><span class="line">              <span class="attr">Resource:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">arn:aws:s3:::dataeng-landing-zone-etl/*</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">arn:aws:s3:::dataeng-landing-zone-etl</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">arn:aws:s3:::dataeng-clean-zone-etl/*</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">arn:aws:s3:::dataeng-clean-zone-etl</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">Effect:</span> <span class="string">Allow</span></span><br><span class="line">              <span class="attr">Action:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">glue:*</span></span><br><span class="line">              <span class="attr">Resource:</span> <span class="string">&#x27;*&#x27;</span></span><br><span class="line">  <span class="attr">Bucket1:</span></span><br><span class="line">    <span class="attr">Type:</span> <span class="string">AWS::S3::Bucket</span></span><br><span class="line">    <span class="attr">Properties:</span></span><br><span class="line">      <span class="attr">VersioningConfiguration:</span></span><br><span class="line">        <span class="attr">Status:</span> <span class="string">Enabled</span></span><br><span class="line">      <span class="attr">BucketEncryption:</span></span><br><span class="line">        <span class="attr">ServerSideEncryptionConfiguration:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">ServerSideEncryptionByDefault:</span></span><br><span class="line">              <span class="attr">SSEAlgorithm:</span> <span class="string">AES256</span></span><br><span class="line">  <span class="attr">BucketPolicy1:</span></span><br><span class="line">    <span class="attr">Type:</span> <span class="string">AWS::S3::BucketPolicy</span></span><br><span class="line">    <span class="attr">Properties:</span></span><br><span class="line">      <span class="attr">Bucket:</span> <span class="string">Bucket1</span></span><br><span class="line">      <span class="attr">PolicyDocument:</span></span><br><span class="line">        <span class="attr">Statement:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">Action:</span> <span class="string">s3:*</span></span><br><span class="line">            <span class="attr">Effect:</span> <span class="string">Deny</span></span><br><span class="line">            <span class="attr">Principal:</span> <span class="string">&#x27;*&#x27;</span></span><br><span class="line">            <span class="attr">Resource:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="string">arn:aws:s3:::Bucket1/*</span></span><br><span class="line">              <span class="bullet">-</span> <span class="string">arn:aws:s3:::Bucket1</span></span><br><span class="line">            <span class="attr">Condition:</span></span><br><span class="line">              <span class="attr">Bool:</span></span><br><span class="line">                <span class="attr">aws:SecureTransport:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<h2 id="What-does-the-YAML-of-the-Lambda-Function-do"><a href="#What-does-the-YAML-of-the-Lambda-Function-do" class="headerlink" title="What does the YAML of the Lambda Function do?"></a>What does the YAML of the Lambda Function do?</h2><p>This YAML file is an AWS Serverless Application Model (SAM) template that defines an AWS Lambda function and its associated resources. Here’s a concise explanation of its steps:</p>
<ol>
<li><p><strong>Template Specification</strong>: Defines the SAM template version and the transformation type, setting the framework for serverless application deployment on AWS.</p>
</li>
<li><p><strong>Lambda Function Configuration (<code>KinesisFirehoseConsume</code>)</strong>:</p>
<ul>
<li>Specifies the function’s code location, runtime environment (Python 3.8), memory allocation (128 MB), and timeout limit (600 seconds).</li>
<li>Configures event invocation settings, including maximum age of events (21600 seconds) and retry attempts (2).</li>
<li>Sets ephemeral storage size to 512 MB for temporary data.</li>
<li>Defines an event trigger from an S3 bucket (<code>Bucket1</code>). The function triggers on object creation events in the specified S3 key prefix (<code>streaming/2023/12/</code>).</li>
<li>Includes AWS Data Wrangler as a Lambda layer for data processing (already set up in <a href="/2023/12/19/AWSLambdaETL/" title="AWS Lambda ETL">AWS Lambda ETL</a>.).</li>
<li>Applies runtime management and packaging configurations.</li>
</ul>
</li>
<li><p><strong>IAM Policy for Lambda (<code>Policies</code>)</strong>:</p>
<ul>
<li>Grants permissions for logging actions, full access to specified S3 buckets (both landing and clean zones), and all actions on AWS Glue services.</li>
</ul>
</li>
<li><p><strong>S3 Bucket Resource (<code>Bucket1</code>)</strong>:</p>
<ul>
<li>Creates an S3 bucket with versioning enabled and AES256 server-side encryption.</li>
</ul>
</li>
<li><p><strong>S3 Bucket Policy (<code>BucketPolicy1</code>)</strong>:</p>
<ul>
<li>Defines a policy for <code>Bucket1</code> to deny all actions if the data transfer is not secured (i.e., not using HTTPS).</li>
</ul>
</li>
</ol>
<p>This template effectively sets up a Lambda function for processing data from an S3 bucket, with security and logging considerations, and the necessary permissions to interact with other AWS services like S3 and Glue.</p>
<p>Here is a screenshot for the Lambda setup: </p>
<p><img src="/2023/12/27/StreamIngestion/KinesisFirehoseConsume.png"></p>
<h2 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h2><p>The data entries are generated using Kinesis Data Generator with the following template, see for <a class="link"   href="https://github.com/KaiLiPlayground/Data-Engineering-with-AWS-experiments/tree/main/Chapter06#configuring-amazon-kinesis-data-generator-kdg" >KDG generator<i class="fas fa-external-link-alt"></i></a> more details: </p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;timestamp&quot;</span><span class="punctuation">:</span><span class="string">&quot;&#123;&#123;date.now&#125;&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;eventType&quot;</span><span class="punctuation">:</span><span class="string">&quot;&#123;&#123;random.weightedArrayElement(</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            &quot;</span>weights<span class="string">&quot;: [0.3,0.1,0.6],</span></span><br><span class="line"><span class="string">            &quot;</span>data<span class="string">&quot;: [&quot;</span>rent<span class="string">&quot;,&quot;</span>buy<span class="string">&quot;,&quot;</span>trailer<span class="string">&quot;]</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    )&#125;&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;film_id&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span><span class="punctuation">&#123;</span>random.number(</span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;min&quot;</span><span class="punctuation">:</span><span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;max&quot;</span><span class="punctuation">:</span><span class="number">1000</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">    )<span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;distributor&quot;</span><span class="punctuation">:</span><span class="string">&quot;&#123;&#123;random.arrayElement(</span></span><br><span class="line"><span class="string">        [&quot;</span>amazon prime<span class="string">&quot;, &quot;</span>google play<span class="string">&quot;, &quot;</span>apple itunes<span class="string">&quot;,&quot;</span>vudo<span class="string">&quot;, &quot;</span>fandango now<span class="string">&quot;, &quot;</span>microsoft<span class="string">&quot;, &quot;</span>youtube<span class="string">&quot;]</span></span><br><span class="line"><span class="string">    )&#125;&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;platform&quot;</span><span class="punctuation">:</span><span class="string">&quot;&#123;&#123;random.arrayElement(</span></span><br><span class="line"><span class="string">        [&quot;</span>ios<span class="string">&quot;, &quot;</span>android<span class="string">&quot;, &quot;</span>xbox<span class="string">&quot;, &quot;</span>playstation<span class="string">&quot;, &quot;</span>smarttv<span class="string">&quot;, &quot;</span>other<span class="string">&quot;]</span></span><br><span class="line"><span class="string">    )&#125;&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;state&quot;</span><span class="punctuation">:</span><span class="string">&quot;&#123;&#123;address.state&#125;&#125;&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h2 id="Expectation"><a href="#Expectation" class="headerlink" title="Expectation"></a>Expectation</h2><p>If all steps are followed correctly, we should be able to query the <code>dataeng_firehose_streaming</code> table entries with AWS Athena. See the below image for reference: </p>
<p><img src="/2023/12/27/StreamIngestion/athena_query_page.png"></p>
<h2 id="Benefits-of-the-Approach"><a href="#Benefits-of-the-Approach" class="headerlink" title="Benefits of the Approach"></a>Benefits of the Approach</h2><p>This method provided several key advantages:</p>
<ul>
<li><strong>Flexibility and Control</strong>: Using Lambda allowed for more control over the data processing and transformation stages.</li>
<li><strong>Efficiency in Data Handling</strong>: The Parquet format ensured that the data was stored in a compressed, optimized format for analysis.</li>
<li><strong>Simplified Cataloging</strong>: AWS Data Wrangler streamlined the process of cataloging data, bypassing the complexity of the Glue Crawler setup.</li>
</ul>
]]></content>
      <categories>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>AWS Kinesis Firehose</tag>
        <tag>AWS Streaming Ingestion</tag>
      </tags>
  </entry>
  <entry>
    <title>Orchestration Pipeline - Project 1</title>
    <url>/2023/12/28/OrchestrationPipeline/</url>
    <content><![CDATA[<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>This project is to make use of cloudformation stack to create codepipeline that will free us from clicking the UI to create&#x2F;define resources for the intent of this project which involves using services: Lambda Functions, SNS Topic, and Step functions.</p>
<p>The goal is to achieve the following: </p>
<ul>
<li><strong>Code Repository</strong>: Use AWS CodeCommit or a compatible version control system to store the application and CloudFormation templates.</li>
<li><strong>Deployment Automation</strong>:<ul>
<li>Use AWS CodePipeline to orchestrate the whole CI&#x2F;CD process.</li>
<li>Set up a source stage pointing to the repository.</li>
<li>Include a deploy stage to deploy CloudFormation stacks that instantiate the infrastructure.</li>
</ul>
</li>
</ul>
<blockquote>
<p>Project Git Repo: <a class="link"   href="https://github.com/KaiLiPlayground/Data_Pipeline_Orchestration" >Data_Pipeline_Orchestration<i class="fas fa-external-link-alt"></i></a></p>
</blockquote>
<h2 id="Pipeline-Workflow"><a href="#Pipeline-Workflow" class="headerlink" title="Pipeline Workflow"></a>Pipeline Workflow</h2><p><img src="/2023/12/28/OrchestrationPipeline/cicd_workflow.png">  </p>
<h2 id="Create-the-Master-Codepipeline-Stack"><a href="#Create-the-Master-Codepipeline-Stack" class="headerlink" title="Create the Master Codepipeline Stack"></a>Create the Master Codepipeline Stack</h2><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">aws cloudformation create<span class="literal">-stack</span> `</span><br><span class="line">      <span class="literal">--stack-name</span> OrchetrationStack `</span><br><span class="line">      <span class="literal">--template-body</span> file://&lt;Path to enter&gt;\orchestrationCP.yaml ` </span><br><span class="line">      <span class="literal">--parameters</span> `</span><br><span class="line">          ParameterKey=CodeCommitRepoName,ParameterValue=p1<span class="literal">-orchestration</span> `</span><br><span class="line">          ParameterKey=BucketName,ParameterValue=dataeng<span class="literal">-clean-zone-etl</span> `</span><br><span class="line">      <span class="literal">--capabilities</span> CAPABILITY_IAM CAPABILITY_NAMED_IAM</span><br></pre></td></tr></table></figure>



<h2 id="What-does-the-original-project-do"><a href="#What-does-the-original-project-do" class="headerlink" title="What does the original project do?"></a>What does the original project do?</h2><p>This project demonstrates how to orchestrate a data pipeline using AWS Step Function, integrating AWS Lambda functions and Amazon SNS for process management and error notifications. The key steps and the purpose of the project are summarized as follows:</p>
<ul>
<li><p><strong>Orchestration with AWS Step Function</strong>:</p>
<ul>
<li>Introduces AWS Step Function for orchestrating data pipelines, suitable for both simple and complex workflows.</li>
<li>Utilizes Lambda functions to process data; however, in larger production pipelines, these could be replaced with AWS Glue jobs.</li>
</ul>
</li>
<li><p><strong>Lambda Function for File Type Checking</strong>:</p>
<ul>
<li>Sets up a Lambda function (<code>dataeng-check-file-ext</code>) to determine the file type of items uploaded to an S3 bucket.</li>
<li>The function analyzes the file extension and passes this information to the Step Function state machine.</li>
</ul>
</li>
<li><p><strong>Lambda Function for Simulating Failures</strong>:</p>
<ul>
<li>Another Lambda function (<code>dataeng-random-failure-generator</code>) randomly generates failures to simulate real-world scenarios in data processing.</li>
</ul>
</li>
<li><p><strong>SNS Topic for Failure Notifications</strong>:</p>
<ul>
<li>Establishes an Amazon SNS topic (<code>dataeng-failure-notification</code>) to send email notifications in case of processing failures.</li>
</ul>
</li>
<li><p><strong>Step Function State Machine Configuration</strong>:</p>
<ul>
<li>Designs a state machine that starts with checking the file extension and then moves to a CHOICE state.</li>
<li>Processes supported file types (.csv) and uses SNS notifications for unsupported types or processing errors.</li>
</ul>
</li>
<li><p><strong>Event-Driven Workflow Setup</strong>:</p>
<ul>
<li>Configures AWS CloudTrail and Amazon EventBridge to trigger the state machine upon new file uploads to an S3 bucket.</li>
</ul>
</li>
<li><p><strong>Testing the Pipeline</strong>:</p>
<ul>
<li>Involves uploading files to the S3 bucket and observing the state machine’s response, including handling both successful and failed executions.</li>
</ul>
</li>
</ul>
<p>Purpose of the project:</p>
<ul>
<li>The project aims to provide hands-on experience in building and managing an event-driven, serverless data pipeline on AWS.</li>
<li>It illustrates the integration of various AWS services, error handling, and the importance of observability in pipeline orchestration.</li>
<li>The exercise serves as a practical example of how AWS Step Function can streamline data pipeline workflows and handle complexities in real-world data engineering scenarios.</li>
</ul>
<blockquote>
<p>For more info, <a class="link"   href="https://subscription.packtpub.com/book/data/9781800560413/8/ch08lvl1sec52/hands-on-ingesting-streaming-data?_gl=1*17mtlck*_gcl_au*MTI5MjU4OTg2MC4xNzAzNDg0Mzg5*_ga*MjExODIxOTUyMS4xNzAzNDg0Mzg5*_ga_Q4R8G7SJDK*MTcwMzgyMDk4NC40LjAuMTcwMzgyMDk4NC42MC4wLjA." >hands-on Orchestration with step functions<i class="fas fa-external-link-alt"></i></a></p>
</blockquote>
]]></content>
      <categories>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>AWS CodePipeline</tag>
        <tag>AWS Step Function Orchestration</tag>
      </tags>
  </entry>
</search>
