<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Managing Fine-grained Permission with AWS Lake Formation</title>
    <url>/2023/12/25/lakeformation/</url>
    <content><![CDATA[<h1 id="Granting-Lake-Formation-Permissions"><a href="#Granting-Lake-Formation-Permissions" class="headerlink" title="Granting Lake Formation Permissions"></a>Granting Lake Formation Permissions</h1><h3 id="Prerequite"><a href="#Prerequite" class="headerlink" title="Prerequite"></a>Prerequite</h3><p>Following the instruction of <a class="link"   href="https://subscription.packtpub.com/book/data/9781804614426/5/ch05lvl1sec33/hands-on-configuring-lake-formation-permissions" >LakeFormation permissions<i class="fas fa-external-link-alt"></i></a>, the intended IAM policies to manage fine-grained access control for data lake has been configured. We now use transition to using Lake Formation to manage fine-grained permission on data lake object. </p>
<h2 id="Motivation-and-Issue"><a href="#Motivation-and-Issue" class="headerlink" title="Motivation and Issue"></a>Motivation and Issue</h2><p>Our goal is to experiment with the granular permission data acess on the column level access of a table in Lake Formation. </p>
<p>At the time of writing this post, the data filter row access UI button is not working, see the below image for reference: </p>
<p><img src="/2023/12/25/lakeformation/dataFilter.png" alt="datafilter"></p>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>We can use AWS CLI as a workaround to UI button issue to allow principal user accessing one specifc column only while blocking the others and allow access to all rows. Here the sample AWS CLI command: </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">aws lakeformation grant-permissions \</span><br><span class="line">--principal DataLakePrincipalIdentifier=arn:aws:iam::&lt;account-id&gt;:user/database-user&quot; \</span><br><span class="line">--resource &#x27;&#123;&quot;TableWithColumns&quot;:&#123;&quot;DatabaseName&quot;:&quot;testdb&quot;,&quot;Name&quot;:&quot;csvparquet&quot;,&quot;ColumnNames&quot;:[&quot;name&quot;]&#125;&#125;&#x27; \</span><br><span class="line">--permissions &quot;SELECT&quot; \</span><br><span class="line">--permissions-with-grant-option &quot;SELECT&quot; \</span><br><span class="line">--profile &quot;&lt;Profile Name&gt;&quot;</span><br></pre></td></tr></table></figure>

<p>In this command, we are allowing the specified principal user to have access to column <code>name</code> but restricting others. The <code>csvparquet</code> table has columns, <code>name</code> and <code>favorite_num</code>. The expected output for querying against the table using the specified principal user is that only column <code>name</code> is showing, see the below image for reference: </p>
<p><img src="/2023/12/25/lakeformation/athenaquery.png"></p>
]]></content>
      <categories>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>AWS LakeFormation</tag>
        <tag>Fine-grained Permission</tag>
      </tags>
  </entry>
  <entry>
    <title>AWS Lambda ETL</title>
    <url>/2023/12/19/AWSLambdaETL/</url>
    <content><![CDATA[<h1 id="Triggering-an-AWS-Lambda-function-when-a-new-file-arrives-in-an-S3-bucket"><a href="#Triggering-an-AWS-Lambda-function-when-a-new-file-arrives-in-an-S3-bucket" class="headerlink" title="Triggering an AWS Lambda function when a new file arrives in an S3 bucket"></a>Triggering an AWS Lambda function when a new file arrives in an S3 bucket</h1><h3 id="Creating-a-Lambda-layer-containeing-the-AWS-Data-Wrangler-library"><a href="#Creating-a-Lambda-layer-containeing-the-AWS-Data-Wrangler-library" class="headerlink" title="Creating a Lambda layer containeing the AWS Data Wrangler library"></a>Creating a Lambda layer containeing the AWS Data Wrangler library</h3><h4 id="Layer-Creation"><a href="#Layer-Creation" class="headerlink" title="Layer Creation"></a>Layer Creation</h4><p><img src="/2023/12/19/AWSLambdaETL/layerCreation.png"></p>
<ul>
<li>upload <a class="link"   href="https://github.com/aws/aws-sdk-pandas/releases" >awsdatawrangler<i class="fas fa-external-link-alt"></i></a> to AWS layer and name it as <code>awsDataWrangler342_py38</code></li>
</ul>
<h4 id="Create-two-new-Amazon-S3-buckets"><a href="#Create-two-new-Amazon-S3-buckets" class="headerlink" title="Create two new Amazon S3 buckets"></a>Create two new Amazon S3 buckets</h4><ul>
<li><p>landing zone S3 bucket: <code>dataeng-landing-zone-etl</code></p>
</li>
<li><p>clean zone S3 bucket: <code>dataeng-clean-zone-etl</code></p>
</li>
</ul>
<h4 id="Create-an-IAM-policy-and-role-for-your-Lambda-function"><a href="#Create-an-IAM-policy-and-role-for-your-Lambda-function" class="headerlink" title="Create an IAM policy and role for your Lambda function"></a>Create an IAM policy and role for your Lambda function</h4><ul>
<li><p>name the policy as <code>DataEngLambdaS3CWGluePolicy</code> </p>
</li>
<li><p>Note: remember to replace the two storage names to your own ones</p>
</li>
</ul>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;Version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2012-10-17&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;Statement&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;logs:PutLogEvents&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;logs:CreateLogGroup&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;logs:CreateLogStream&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;arn:aws:logs:*:*:*&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;s3:*&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;arn:aws:s3:::dataeng-landing-zone-etl/*&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;arn:aws:s3:::dataeng-landing-zone-etl&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;arn:aws:s3:::dataeng-clean-zone-etl/*&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;arn:aws:s3:::dataeng-clean-zone-etl&quot;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;Effect&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Allow&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;Action&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;glue:*&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;Resource&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Create a custom role and name it as <code>DataEngLambdaS3CWGluePolicy</code> using <code>DataEngLambdaS3CWGluePolicy</code></li>
</ul>
<h4 id="Create-a-Lambda-function"><a href="#Create-a-Lambda-function" class="headerlink" title="Create a Lambda function"></a>Create a Lambda function</h4><ul>
<li>specify the Python version and your custom role in this function, see the below screenshot for reference</li>
<li>name it as <code>CSVtoParquetLambda</code></li>
</ul>
<p><img src="/2023/12/19/AWSLambdaETL/csvtoparquetlambda.png"></p>
<ul>
<li><p>add the layer (<code>awsDataWrangler342_py38</code>) created into the lambda function</p>
</li>
<li><p>add the code source which is given as below</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> boto3</span><br><span class="line"><span class="keyword">import</span> awswrangler <span class="keyword">as</span> wr</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> unquote_plus</span><br><span class="line"></span><br><span class="line"><span class="comment"># this &#x27;debug&#x27; logging part can be removed as it is </span></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.getLogger(<span class="string">&#x27;awswrangler&#x27;</span>).setLevel(logging.DEBUG)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lambda_handler</span>(<span class="params">event, context</span>):</span><br><span class="line">    <span class="comment"># Get the source bucket and object name as passed to the Lambda function</span></span><br><span class="line">    <span class="keyword">for</span> record <span class="keyword">in</span> event[<span class="string">&#x27;Records&#x27;</span>]:</span><br><span class="line">        bucket = record[<span class="string">&#x27;s3&#x27;</span>][<span class="string">&#x27;bucket&#x27;</span>][<span class="string">&#x27;name&#x27;</span>]</span><br><span class="line">        key = unquote_plus(record[<span class="string">&#x27;s3&#x27;</span>][<span class="string">&#x27;object&#x27;</span>][<span class="string">&#x27;key&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We will set the DB and table name based on the last two elements of </span></span><br><span class="line">    <span class="comment"># the path prior to the file name. If key = &#x27;dms/sakila/film/LOAD01.csv&#x27;,</span></span><br><span class="line">    <span class="comment"># then the following lines will set db to sakila and table_name to &#x27;film&#x27;</span></span><br><span class="line">    key_list = key.split(<span class="string">&quot;/&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;key_list: <span class="subst">&#123;key_list&#125;</span>&#x27;</span>)</span><br><span class="line">    db_name = key_list[<span class="built_in">len</span>(key_list)-<span class="number">3</span>]</span><br><span class="line">    table_name = key_list[<span class="built_in">len</span>(key_list)-<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Bucket: <span class="subst">&#123;bucket&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Key: <span class="subst">&#123;key&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;DB Name: <span class="subst">&#123;db_name&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Table Name: <span class="subst">&#123;table_name&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    input_path = <span class="string">f&quot;s3://<span class="subst">&#123;bucket&#125;</span>/<span class="subst">&#123;key&#125;</span>&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Input_Path: <span class="subst">&#123;input_path&#125;</span>&#x27;</span>)</span><br><span class="line">    output_path = <span class="string">f&quot;s3://dataeng-clean-zone-etl/<span class="subst">&#123;db_name&#125;</span>/<span class="subst">&#123;table_name&#125;</span>&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Output_Path: <span class="subst">&#123;output_path&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    input_df = wr.s3.read_csv([input_path])</span><br><span class="line"></span><br><span class="line">    current_databases = wr.catalog.databases()</span><br><span class="line">    wr.catalog.databases()</span><br><span class="line">    <span class="keyword">if</span> db_name <span class="keyword">not</span> <span class="keyword">in</span> current_databases.values:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;- Database <span class="subst">&#123;db_name&#125;</span> does not exist ... creating&#x27;</span>)</span><br><span class="line">        wr.catalog.create_database(db_name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;- Database <span class="subst">&#123;db_name&#125;</span> already exists&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    result = wr.s3.to_parquet(</span><br><span class="line">        df=input_df, </span><br><span class="line">        path=output_path, </span><br><span class="line">        dataset=<span class="literal">True</span>,</span><br><span class="line">        database=db_name,</span><br><span class="line">        table=table_name,</span><br><span class="line">        mode=<span class="string">&quot;append&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;RESULT: &quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;result&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<ul>
<li>Last Step: deploy</li>
</ul>
<blockquote>
<p>Note: </p>
<p><code>logging.getLogger(&#39;awswrangler&#39;).setLevel(logging.DEBUG)</code> is to enable debug log to check if anything can go wrong from the awswrangler library. </p>
</blockquote>
<h4 id="Trigger-configuration"><a href="#Trigger-configuration" class="headerlink" title="Trigger configuration"></a>Trigger configuration</h4><ul>
<li>configurate trigger to listen on landing S3 bucket (<code>dataeng-landing-zone-etl</code>) when a new file (suffix with <code>.csv</code>) arrives</li>
</ul>
<p><img src="/2023/12/19/AWSLambdaETL/triggers.png"></p>
<ul>
<li>create a CSV file in your local file system with the below sample fields and values</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Name,favorite_num</span><br><span class="line">Vrinda,22</span><br><span class="line">Tracy,28</span><br><span class="line">Gareth,23</span><br><span class="line">Chris,16</span><br><span class="line">Emma,14</span><br><span class="line">Carlos,7</span><br><span class="line">Cooper,11</span><br><span class="line">Praful,4</span><br><span class="line">David,33</span><br><span class="line">Shilpa,2</span><br><span class="line">Gary,18</span><br><span class="line">Sean,20</span><br><span class="line">Ha-yoon,9</span><br><span class="line">Elizabeth,8</span><br><span class="line">Mary,1</span><br><span class="line">Chen,15</span><br><span class="line">Janet,22</span><br><span class="line">Mariusz,25</span><br><span class="line">Romain,11</span><br><span class="line">Matt,25</span><br><span class="line">Brendan,19</span><br><span class="line">Roger,2</span><br><span class="line">Jack,7</span><br><span class="line">Sachin,17</span><br><span class="line">Francisco,5</span><br></pre></td></tr></table></figure>

<ul>
<li>upload the CSV file to the remote bucket <code>dataeng-landing-zone-etl</code> using <code>aws</code> CLI</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">aws s3 cp .\test.csv s3://dataeng-landing-zone-etl/testdb/csvparquet/test.csv</span><br></pre></td></tr></table></figure>

<ul>
<li>list the parquet file in the clean zone bucket <code>dataeng-clean-zone-etl</code> if everything done correctly</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">aws s3 ls s3://dataeng-clean-zone-etl/testdb/csvparquet/</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># expected output</span></span> </span><br><span class="line">2023-12-19 16:03:57       2199 d5d09abfbfda4117b544a0bca1f8a134.snappy.parquet</span><br></pre></td></tr></table></figure>

<h4 id="Possible-Issues"><a href="#Possible-Issues" class="headerlink" title="Possible Issues"></a>Possible Issues</h4><ul>
<li>Lamdba function execution timeout shown in CloudWatch log</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1702972039080,&quot;[DEBUG]    2023-12-19T07:47:19.080Z    Applying default config argument verify with value None.</span><br><span class="line">&quot;</span><br><span class="line">1702972040916,&#x27;&quot;- Database testdb already exists</span><br><span class="line">&quot;</span><br><span class="line">1702972040937,&quot;[DEBUG]    2023-12-19T07:47:20.937Z    Applying default config argument botocore_config with value None.</span><br><span class="line">&quot;</span><br><span class="line">1702972040937,&quot;[DEBUG]    2023-12-19T07:47:20.937Z    Applying default config argument verify with value None.</span><br><span class="line">&quot;</span><br><span class="line">1702972043538,&quot;[DEBUG]    2023-12-19T07:47:23.518Z    Applying default config argument botocore_config with value None.</span><br><span class="line">&quot;</span><br><span class="line">1702972043538,&quot;[DEBUG]    2023-12-19T07:47:23.538Z    Applying default config argument verify with value None.</span><br><span class="line">&quot;</span><br><span class="line">1702972214725,&quot;2023-12-19T07:50:14.725Z Task timed out after 182.10 seconds</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Based on the code in the Lambda function, we know Database <code>testdb</code> exists is harmless and expected. But the execution got timed out afterwards. Initially, the function was set at 3 minutes, although the CSV file was only 128 bytes in my case, it still got timed out. </p>
</blockquote>
<ul>
<li><p>Solution</p>
<ul>
<li>extend the execution timeout in the Lambda function in <strong>Configuration</strong> to 10 minutes</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2023-12-19T16:03:53.249+08:00    START RequestId: 67114d4d-25cf-42f2-xxx6e6 Version: $LATEST </span><br><span class="line"></span><br><span class="line"># omitted the intermediate lines </span><br><span class="line"></span><br><span class="line">2023-12-19T16:03:56.713+08:00    &#123;&#x27;paths&#x27;: [&#x27;s3://dataeng-clean-zone-etl/testdb/csvparquet/d5d09abfbfda4117b544a0bca1f8a134.snappy.parquet&#x27;], &#x27;partitions_values&#x27;: &#123;&#125;&#125;</span><br><span class="line">2023-12-19T16:03:56.715+08:00    END RequestId: 67114d4d-25cf-42f2-be60-xxx6e6 </span><br></pre></td></tr></table></figure>

<blockquote>
<p>After examing the time duration for the new Lambda run based on the 10-minutes timeout Lambda definition, our Lambda run completed without error. </p>
<p>And the time duration for the run went for over 3 minutes. That is why our earlier run duration indeed went over the duration of 3 minutes timeout limit. </p>
<p>One possible reason for this is the latency for establishing IO across buckets in AWS. </p>
<p>Although the debug logging level set did not help in this case for troubleshooting, but it could be a great use for future debugging scenarios if something goes wrong with the custom Python script. </p>
</blockquote>
]]></content>
      <categories>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>AWS</tag>
        <tag>Lambda</tag>
        <tag>ETL</tag>
      </tags>
  </entry>
</search>
